{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return [ word for (word,tag) in blob.tags if tag == \"JJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_statuses</th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>retweets</th>\n",
       "      <th>location</th>\n",
       "      <th>created</th>\n",
       "      <th>followers</th>\n",
       "      <th>is_user_verified</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>language</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.730776e+07</td>\n",
       "      <td>50931</td>\n",
       "      <td>AppelquistP</td>\n",
       "      <td>Damn I thought I was finally in the #Matrix or...</td>\n",
       "      <td>0</td>\n",
       "      <td>OZ</td>\n",
       "      <td>18/06/2021</td>\n",
       "      <td>130</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#blackmirror</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#blackmirror.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.880588e+09</td>\n",
       "      <td>27578</td>\n",
       "      <td>kaarnama13</td>\n",
       "      <td>â€œNosediveâ€ episode from #blackmirror gets ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Aage se right</td>\n",
       "      <td>18/06/2021</td>\n",
       "      <td>669</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#blackmirror</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#blackmirror.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.370116e+09</td>\n",
       "      <td>115767</td>\n",
       "      <td>Niggvtivity</td>\n",
       "      <td>I've just watched episode S03 | E05 of Black M...</td>\n",
       "      <td>0</td>\n",
       "      <td>Bonneuil-sur-Marne, France</td>\n",
       "      <td>18/06/2021</td>\n",
       "      <td>795</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#blackmirror</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#blackmirror.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.817271e+07</td>\n",
       "      <td>4971</td>\n",
       "      <td>DystopicRedhead</td>\n",
       "      <td>The perfect description of the #Tories and the...</td>\n",
       "      <td>4</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>18/06/2021</td>\n",
       "      <td>2117</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#blackmirror</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#blackmirror.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.402080e+18</td>\n",
       "      <td>13</td>\n",
       "      <td>smash_media_us</td>\n",
       "      <td>ã€ #ONEUS Ã— #smash. ã€‘\\n\\nONEUS is on now s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18/06/2021</td>\n",
       "      <td>762</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#blackmirror</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#blackmirror.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>1.243260e+18</td>\n",
       "      <td>600</td>\n",
       "      <td>CatheGrisson</td>\n",
       "      <td>It's getting harder every day to be a fan of @...</td>\n",
       "      <td>0</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>17/06/2021</td>\n",
       "      <td>146</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#thewalkingdead</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#thewalkingdead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>2.552374e+08</td>\n",
       "      <td>26940</td>\n",
       "      <td>DavidOpie</td>\n",
       "      <td>#BlackSummer season 2 is out today on @Netflix...</td>\n",
       "      <td>0</td>\n",
       "      <td>London, England</td>\n",
       "      <td>17/06/2021</td>\n",
       "      <td>5467</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#thewalkingdead</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#thewalkingdead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>1.063830e+18</td>\n",
       "      <td>1932</td>\n",
       "      <td>Alicia05021</td>\n",
       "      <td>Are we even getting a trailer or are we just g...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/06/2021</td>\n",
       "      <td>83</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#thewalkingdead</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#thewalkingdead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1.393000e+18</td>\n",
       "      <td>315</td>\n",
       "      <td>onterrorstreet</td>\n",
       "      <td>#TheWalkingDead Final Season Premiere Reveal B...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/06/2021</td>\n",
       "      <td>220</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#thewalkingdead</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#thewalkingdead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>4.074545e+08</td>\n",
       "      <td>15882</td>\n",
       "      <td>NerdsThatGeek</td>\n",
       "      <td>Check it out fans of #TheWalkingDead! Here are...</td>\n",
       "      <td>1</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>17/06/2021</td>\n",
       "      <td>4640</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#thewalkingdead</td>\n",
       "      <td>en</td>\n",
       "      <td>date post corona\\2021-06-18_en_#thewalkingdead...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1735 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id  user_statuses             name  \\\n",
       "0     2.730776e+07          50931      AppelquistP   \n",
       "1     2.880588e+09          27578       kaarnama13   \n",
       "2     2.370116e+09         115767      Niggvtivity   \n",
       "3     4.817271e+07           4971  DystopicRedhead   \n",
       "4     1.402080e+18             13   smash_media_us   \n",
       "...            ...            ...              ...   \n",
       "1730  1.243260e+18            600     CatheGrisson   \n",
       "1731  2.552374e+08          26940        DavidOpie   \n",
       "1732  1.063830e+18           1932      Alicia05021   \n",
       "1733  1.393000e+18            315   onterrorstreet   \n",
       "1734  4.074545e+08          15882    NerdsThatGeek   \n",
       "\n",
       "                                                  tweet  retweets  \\\n",
       "0     Damn I thought I was finally in the #Matrix or...         0   \n",
       "1     â€œNosediveâ€ episode from #blackmirror gets ...         0   \n",
       "2     I've just watched episode S03 | E05 of Black M...         0   \n",
       "3     The perfect description of the #Tories and the...         4   \n",
       "4     ã€ #ONEUS Ã— #smash. ã€‘\\n\\nONEUS is on now s...         0   \n",
       "...                                                 ...       ...   \n",
       "1730  It's getting harder every day to be a fan of @...         0   \n",
       "1731  #BlackSummer season 2 is out today on @Netflix...         0   \n",
       "1732  Are we even getting a trailer or are we just g...         0   \n",
       "1733  #TheWalkingDead Final Season Premiere Reveal B...         1   \n",
       "1734  Check it out fans of #TheWalkingDead! Here are...         1   \n",
       "\n",
       "                        location     created  followers  is_user_verified  \\\n",
       "0                             OZ  18/06/2021        130             False   \n",
       "1                  Aage se right  18/06/2021        669             False   \n",
       "2     Bonneuil-sur-Marne, France  18/06/2021        795             False   \n",
       "3                 United Kingdom  18/06/2021       2117             False   \n",
       "4                            NaN  18/06/2021        762             False   \n",
       "...                          ...         ...        ...               ...   \n",
       "1730               Las Vegas, NV  17/06/2021        146             False   \n",
       "1731             London, England  17/06/2021       5467             False   \n",
       "1732                         NaN  17/06/2021         83             False   \n",
       "1733                         NaN  17/06/2021        220             False   \n",
       "1734                  Denver, CO  17/06/2021       4640             False   \n",
       "\n",
       "      favorite_count  in_reply_to_status_id          keyword language  \\\n",
       "0                  0                    NaN     #blackmirror       en   \n",
       "1                  6                    NaN     #blackmirror       en   \n",
       "2                  0                    NaN     #blackmirror       en   \n",
       "3                  8                    NaN     #blackmirror       en   \n",
       "4                  5                    NaN     #blackmirror       en   \n",
       "...              ...                    ...              ...      ...   \n",
       "1730               2                    NaN  #thewalkingdead       en   \n",
       "1731               0                    NaN  #thewalkingdead       en   \n",
       "1732               0                    NaN  #thewalkingdead       en   \n",
       "1733               9                    NaN  #thewalkingdead       en   \n",
       "1734               3                    NaN  #thewalkingdead       en   \n",
       "\n",
       "                                                   site  \n",
       "0      date post corona\\2021-06-18_en_#blackmirror.json  \n",
       "1      date post corona\\2021-06-18_en_#blackmirror.json  \n",
       "2      date post corona\\2021-06-18_en_#blackmirror.json  \n",
       "3      date post corona\\2021-06-18_en_#blackmirror.json  \n",
       "4      date post corona\\2021-06-18_en_#blackmirror.json  \n",
       "...                                                 ...  \n",
       "1730  date post corona\\2021-06-18_en_#thewalkingdead...  \n",
       "1731  date post corona\\2021-06-18_en_#thewalkingdead...  \n",
       "1732  date post corona\\2021-06-18_en_#thewalkingdead...  \n",
       "1733  date post corona\\2021-06-18_en_#thewalkingdead...  \n",
       "1734  date post corona\\2021-06-18_en_#thewalkingdead...  \n",
       "\n",
       "[1735 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(r'C:\\Users\\nitis\\OneDrive\\Dokumente\\movie_sentiment\\aftercorona.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a spark session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.0.118:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x181bdb15310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, user_statuses: string, name: string, tweet: string, retweets: string, location: string, created: string, followers: string, is_user_verified: string, favorite_count: string, in_reply_to_status_id: string, keyword: string, language: string, site: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read twitter csv file\n",
    "df_pyspark = spark.read.option('header','true').csv(r'C:\\Users\\nitis\\OneDrive\\Dokumente\\movie_sentiment\\aftercorona.csv', sep=',', multiLine=True)\n",
    "df_pyspark\n",
    "# here it shows all the column and the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+----------+---------+----------------+--------------+---------------------+------------+--------------------+--------------------+\n",
      "|             user_id| user_statuses|                name|               tweet|            retweets|            location|   created|followers|is_user_verified|favorite_count|in_reply_to_status_id|     keyword|            language|                site|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+----------+---------+----------------+--------------+---------------------+------------+--------------------+--------------------+\n",
      "|            27307763|         50931|         AppelquistP|Damn I thought I ...|                   0|                  OZ|18/06/2021|      130|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|          2880588386|         27578|          kaarnama13|â€œNosediveâ€ ep...|                   0|       Aage se right|18/06/2021|      669|           FALSE|             6|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|          2370116357|        115767|         Niggvtivity|I've just watched...|                   0|Bonneuil-sur-Marn...|18/06/2021|      795|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|            48172714|          4971|     DystopicRedhead|The perfect descr...|                   4|      United Kingdom|18/06/2021|     2117|           FALSE|             8|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|         1.40208E+18|            13|      smash_media_us|ã€ #ONEUS Ã— #sm...|                   0|                null|18/06/2021|      762|           FALSE|             5|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|           584767409|          7251|        MarkAWigmore|@BBN_Ireland @Fat...|                   0| South East, England|18/06/2021|       82|           FALSE|             1|             1.41E+18|#blackmirror|                  en|date post corona\\...|\n",
      "|           1.046E+18|           483|             AShehed|Imagining if TikT...|                   0|     London, England|18/06/2021|       46|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|            17997001|          1818|      edieeverywhere|@ArshadParvez Nei...|                   0|    Atlanta, GA, USA|18/06/2021|      549|           FALSE|             0|             1.40E+18|#blackmirror|                  en|date post corona\\...|\n",
      "|            22957037|        153444|         PlayboyBPAC|#UK #Canada #Fran...|                   1|Your girl house o...|18/06/2021|      846|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|         1.12973E+18|          1631|             CMehtra|\"I have been watc...|                   0|                null|18/06/2021|       14|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|            85021117|         12379|     dawnndragonfire|@netflix Please I...|                   0|                null|18/06/2021|       82|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|            15671513|         14225|           SXFoxstar|So I've finally g...|                   0|    Chester, England|17/06/2021|      182|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|           282252332|         35063|        JizzelEtBass|@lockwoodx3 It is...|                   0|         Anaheim, CA|17/06/2021|     1457|           FALSE|             1|             1.41E+18|#blackmirror|                  en|date post corona\\...|\n",
      "|          2770709360|           477|       SunDiegoWraps|Model X will be l...|                   0|                null|17/06/2021|       51|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|         1.19679E+18|          8316|      MissDarkBunnyy|Whatâ€™s your fav...|                   0|                null|17/06/2021|     1271|           FALSE|             2|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|         1.37891E+18|          1256|     littlepigmarket|@ButterHoops Same...|                   0|              Babel |17/06/2021|      824|           FALSE|             1|             1.41E+18|#blackmirror|                  en|date post corona\\...|\n",
      "|            34596198|         15236|         Sandile_S_N|Oh! And #blackmirror|                   0|        South Africa|17/06/2021|      494|           FALSE|             0|             1.41E+18|#blackmirror|                  en|date post corona\\...|\n",
      "|         1.23095E+18|           139|      EnthusiastPark|INSIDE BLACK MIRR...|                   0|                null|17/06/2021|       10|           FALSE|             0|                 null|#blackmirror|                  en|date post corona\\...|\n",
      "|          1391322103|          5559|        WyattMatturs|\"PARALLEL UNIVERS...|                null|                null|      null|     null|            null|          null|                 null|        null|                null|                null|\n",
      "|The ULTIMATE 'Pro...| WRONG PLANETS| 2-Worlds ROCKED!...|                   1|Berkeley, California|          17/06/2021|        58|    FALSE|               0|          null|         #blackmirror|          en|date post corona\\...|                null|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+----------+---------+----------------+--------------+---------------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the dataframe in spark\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the dataframe is spark dataframe and pandas\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id='27307763', user_statuses='50931', name='AppelquistP', tweet='Damn I thought I was finally in the #Matrix or #blackmirror or #Aliens landed and this was their way of a soft opening...\\nNow, I appreciate #Interns even more.. https://t.co/AjdTC8V6tq', retweets='0', location='OZ', created='18/06/2021', followers='130', is_user_verified='FALSE', favorite_count='0', in_reply_to_status_id=None, keyword='#blackmirror', language='en', site='date post corona\\\\2021-06-18_en_#blackmirror.json'),\n",
       " Row(user_id='2880588386', user_statuses='27578', name='kaarnama13', tweet='â€œNosediveâ€\\x9d episode from #blackmirror gets closer to reality with kencil culture', retweets='0', location='Aage se right', created='18/06/2021', followers='669', is_user_verified='FALSE', favorite_count='6', in_reply_to_status_id=None, keyword='#blackmirror', language='en', site='date post corona\\\\2021-06-18_en_#blackmirror.json'),\n",
       " Row(user_id='2370116357', user_statuses='115767', name='Niggvtivity', tweet=\"I've just watched episode S03 | E05 of Black Mirror! #BlackMirror https://t.co/gYm02z0pa4 #tvtime\", retweets='0', location='Bonneuil-sur-Marne, France', created='18/06/2021', followers='795', is_user_verified='FALSE', favorite_count='0', in_reply_to_status_id=None, keyword='#blackmirror', language='en', site='date post corona\\\\2021-06-18_en_#blackmirror.json')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here the head works diffrently too\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_statuses: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- retweets: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- created: string (nullable = true)\n",
      " |-- followers: string (nullable = true)\n",
      " |-- is_user_verified: string (nullable = true)\n",
      " |-- favorite_count: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- keyword: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# like df.info in pandas\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|               tweet|     keyword|\n",
      "+--------------------+------------+\n",
      "|Damn I thought I ...|#blackmirror|\n",
      "|â€œNosediveâ€ ep...|#blackmirror|\n",
      "|I've just watched...|#blackmirror|\n",
      "|The perfect descr...|#blackmirror|\n",
      "|ã€ #ONEUS Ã— #sm...|#blackmirror|\n",
      "|@BBN_Ireland @Fat...|#blackmirror|\n",
      "|Imagining if TikT...|#blackmirror|\n",
      "|@ArshadParvez Nei...|#blackmirror|\n",
      "|#UK #Canada #Fran...|#blackmirror|\n",
      "|\"I have been watc...|#blackmirror|\n",
      "|@netflix Please I...|#blackmirror|\n",
      "|So I've finally g...|#blackmirror|\n",
      "|@lockwoodx3 It is...|#blackmirror|\n",
      "|Model X will be l...|#blackmirror|\n",
      "|Whatâ€™s your fav...|#blackmirror|\n",
      "|@ButterHoops Same...|#blackmirror|\n",
      "|Oh! And #blackmirror|#blackmirror|\n",
      "|INSIDE BLACK MIRR...|#blackmirror|\n",
      "|\"PARALLEL UNIVERS...|        null|\n",
      "|                   1|          en|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.select(['tweet','keyword'])\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o79.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (10.90.0.118 executor driver): java.io.IOException: Cannot run program \"C:\\Users\\nitis\\anaconda3\": CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\Users\\nitis\\anaconda3\": CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-022f8e03e004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mexample_udf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_adjectives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_pyspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_pyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_udf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_pyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_pyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o79.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (10.90.0.118 executor driver): java.io.IOException: Cannot run program \"C:\\Users\\nitis\\anaconda3\": CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\Users\\nitis\\anaconda3\": CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "# to read custom function, we need to import udf which is user defined function\n",
    "example_udf = udf(get_adjectives)\n",
    "df_pyspark = df_pyspark.withColumn('result', example_udf(df_pyspark.tweet))\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
